{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNaez5c5/2ICqJDP3E3ZaBD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stryo2/imagedenoising/blob/main/imgDenoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH8NOyutb4_q",
        "outputId": "166ab219-738a-4c5d-faf7-1f48dfd73d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the ZIP file in your Google Drive\n",
        "zip_file_path = '/content/drive/My Drive/Train.zip'\n",
        "\n",
        "# Directory where the ZIP file will be extracted\n",
        "extract_dir = '/content/extracted_dataset'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "\n",
        "print(\"extraction done!!!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMeaVaFpcFQB",
        "outputId": "344093b3-e22b-4d6d-b291-1b61422add6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extraction done!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the contents of the extracted directory\n",
        "extracted_files = os.listdir(extract_dir)\n",
        "print(\"Extracted files and folders:\", extracted_files)\n",
        "\n",
        "# List the contents of the 'Train' directory\n",
        "train_dir = os.path.join(extract_dir, 'Train')\n",
        "extracted_train_files = os.listdir(train_dir)\n",
        "print(\"Extracted files and folders in Train:\", extracted_train_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oL8lbF0cFSu",
        "outputId": "6e8307fe-dff1-4f63-fc74-540b1ba027cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files and folders: ['Train']\n",
            "Extracted files and folders in Train: ['low', 'high']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FXIb5_NWcYa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, Reshape, Add, Activation, Multiply, concatenate\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "l5zS5OQscFVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "\n",
        "def create_patches(image, patch_size):\n",
        "    patches = []\n",
        "    h, w, _ = image.shape\n",
        "    for i in range(0, h, patch_size):\n",
        "        for j in range(0, w, patch_size):\n",
        "            patch = image[i:i+patch_size, j:j+patch_size]\n",
        "            patches.append(patch)\n",
        "    return np.array(patches)\n",
        "\n",
        "def load_and_patch_images(folder, image_size, patch_size):\n",
        "    patches = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img_path = os.path.join(folder, filename)\n",
        "        try:\n",
        "            img = load_img(img_path, target_size=image_size)\n",
        "            img_array = img_to_array(img)\n",
        "            img_patches = create_patches(img_array, patch_size)\n",
        "            patches.extend(img_patches)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "    return np.array(patches)\n",
        "\n",
        "# Parameters\n",
        "image_size = (512, 512)  # Resize images to this size\n",
        "patch_size = 256  # Patch size\n",
        "\n",
        "# Define paths to image directories\n",
        "high_images_dir = os.path.join(train_dir, 'high')\n",
        "low_images_dir = os.path.join(train_dir, 'low')\n",
        "\n",
        "# Check if directories exist\n",
        "if not os.path.exists(high_images_dir):\n",
        "    print(f\"Directory not found: {high_images_dir}\")\n",
        "if not os.path.exists(low_images_dir):\n",
        "    print(f\"Directory not found: {low_images_dir}\")\n",
        "\n",
        "# Load and patch images if directories exist\n",
        "if os.path.exists(high_images_dir) and os.path.exists(low_images_dir):\n",
        "    X_high = load_and_patch_images(high_images_dir, image_size, patch_size)\n",
        "    X_low = load_and_patch_images(low_images_dir, image_size, patch_size)\n",
        "\n",
        "    # Normalize images\n",
        "    X_high = X_high / 255.0\n",
        "    X_low = X_low / 255.0\n",
        "\n",
        "    # Split into train and test sets (you can adjust the split ratio)\n",
        "    split_ratio = 0.8\n",
        "    split_index = int(len(X_high) * split_ratio)\n",
        "\n",
        "    X_train_patches = X_low[:split_index]\n",
        "    y_train_patches = X_high[:split_index]\n",
        "    X_test_patches = X_low[split_index:]\n",
        "    y_test_patches = X_high[split_index:]\n",
        "\n",
        "    print(f\"Train data shape: {X_train_patches.shape}, {y_train_patches.shape}\")\n",
        "    print(f\"Test data shape: {X_test_patches.shape}, {y_test_patches.shape}\")\n",
        "else:\n",
        "    print(\"One or both directories do not exist. Please check the paths.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8rPC5_ZcFYT",
        "outputId": "e9b298f8-22ae-4f99-f5ce-0f5679098e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/extracted_dataset/Train/high/.DS_Store: cannot identify image file <_io.BytesIO object at 0x7e0ad03aaf70>\n",
            "Error loading image /content/extracted_dataset/Train/low/.DS_Store: cannot identify image file <_io.BytesIO object at 0x7e0ad03ab380>\n",
            "Train data shape: (1552, 256, 256, 3), (1552, 256, 256, 3)\n",
            "Test data shape: (388, 256, 256, 3), (388, 256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataloder(tf.keras.utils.Sequence):\n",
        "    def __init__(self, X, y, batch_size=1, shuffle=False):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(X))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        indexes = self.indexes[i * self.batch_size : (i+1) * self.batch_size]\n",
        "        batch_x = self.X[indexes]\n",
        "        batch_y = self.y[indexes]\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes) // self.batch_size\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.indexes = np.random.permutation(self.indexes)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8\n",
        "train_dataloader = Dataloder(X_train_patches, y_train_patches, batch_size, shuffle=True)\n",
        "test_dataloader = Dataloder(X_test_patches, y_test_patches, batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "LzWfRcE3cFa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EAM(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.conv1 = Conv2D(64, (3,3), dilation_rate=1,padding='same',activation='relu')\n",
        "    self.conv2 = Conv2D(64, (3,3), dilation_rate=2,padding='same',activation='relu')\n",
        "\n",
        "    self.conv3 = Conv2D(64, (3,3), dilation_rate=3,padding='same',activation='relu')\n",
        "    self.conv4 = Conv2D(64, (3,3), dilation_rate=4,padding='same',activation='relu')\n",
        "\n",
        "    self.conv5 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
        "\n",
        "    self.conv6 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
        "    self.conv7 = Conv2D(64, (3,3),padding='same')\n",
        "\n",
        "    self.conv8 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
        "    self.conv9 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
        "    self.conv10 = Conv2D(64, (1,1),padding='same')\n",
        "\n",
        "    self.gap = GlobalAveragePooling2D()\n",
        "\n",
        "    self.conv11 = Conv2D(64, (3,3),padding='same',activation='relu')\n",
        "    self.conv12 = Conv2D(64, (3,3),padding='same',activation='sigmoid')\n",
        "\n",
        "  def call(self,input):\n",
        "    conv1 = self.conv1(input)\n",
        "    conv1 = self.conv2(conv1)\n",
        "\n",
        "    conv2 = self.conv3(input)\n",
        "    conv2 = self.conv4(conv2)\n",
        "\n",
        "    concat = concatenate([conv1,conv2])\n",
        "    conv3 = self.conv5(concat)\n",
        "    add1 = Add()([input,conv3])\n",
        "\n",
        "    conv4 = self.conv6(add1)\n",
        "    conv4 = self.conv7(conv4)\n",
        "    add2 = Add()([conv4,add1])\n",
        "    add2 = Activation('relu')(add2)\n",
        "\n",
        "    conv5 = self.conv8(add2)\n",
        "    conv5 = self.conv9(conv5)\n",
        "    conv5 = self.conv10(conv5)\n",
        "    add3 = Add()([add2,conv5])\n",
        "    add3 = Activation('relu')(add3)\n",
        "\n",
        "    gap = self.gap(add3)\n",
        "    gap = Reshape((1,1,64))(gap)\n",
        "    conv6 = self.conv11(gap)\n",
        "    conv6 = self.conv12(conv6)\n",
        "\n",
        "    mul = Multiply()([conv6, add3])\n",
        "    out = Add()([input,mul])\n",
        "    return out\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    max_pixel = 1.0\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "input = Input(shape=(256, 256, 3))\n",
        "\n",
        "conv1 = Conv2D(64, (3,3),padding='same')(input)\n",
        "eam1 = EAM()(conv1)\n",
        "eam2 = EAM()(eam1)\n",
        "eam3 = EAM()(eam2)\n",
        "eam4 = EAM()(eam3)\n",
        "conv2 = Conv2D(3, (3,3),padding='same')(eam4)\n",
        "out = Add()([conv2,input])\n",
        "\n",
        "RIDNet = Model(input,out)\n",
        "RIDNet.compile(optimizer=tf.keras.optimizers.Adam(1e-03), loss=tf.keras.losses.MeanSquaredError(),metrics=[psnr])"
      ],
      "metadata": {
        "id": "SPULwRizcFdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "RIDNet.fit(train_dataloader, validation_data=test_dataloader, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYzSR5wJcFg0",
        "outputId": "3207a8ba-be11-4bdc-9953-e684f79ca502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "194/194 [==============================] - 344s 2s/step - loss: 0.0377 - psnr: 16.5303 - val_loss: 0.0278 - val_psnr: 17.3093\n",
            "Epoch 2/15\n",
            "194/194 [==============================] - 334s 2s/step - loss: 0.0258 - psnr: 17.8718 - val_loss: 0.0222 - val_psnr: 18.2577\n",
            "Epoch 3/15\n",
            "194/194 [==============================] - 316s 2s/step - loss: 0.0197 - psnr: 19.1108 - val_loss: 0.0188 - val_psnr: 19.0022\n",
            "Epoch 4/15\n",
            "194/194 [==============================] - 315s 2s/step - loss: 0.0192 - psnr: 19.2879 - val_loss: 0.0178 - val_psnr: 19.4708\n",
            "Epoch 5/15\n",
            "194/194 [==============================] - 315s 2s/step - loss: 0.0192 - psnr: 19.2435 - val_loss: 0.0177 - val_psnr: 19.7338\n",
            "Epoch 6/15\n",
            "194/194 [==============================] - 332s 2s/step - loss: 0.0181 - psnr: 19.6381 - val_loss: 0.0172 - val_psnr: 19.7545\n",
            "Epoch 7/15\n",
            "194/194 [==============================] - 315s 2s/step - loss: 0.0175 - psnr: 19.6790 - val_loss: 0.0167 - val_psnr: 20.0072\n",
            "Epoch 8/15\n",
            "194/194 [==============================] - 332s 2s/step - loss: 0.0176 - psnr: 19.7097 - val_loss: 0.0171 - val_psnr: 19.8815\n",
            "Epoch 9/15\n",
            "194/194 [==============================] - 314s 2s/step - loss: 0.0170 - psnr: 19.8851 - val_loss: 0.0181 - val_psnr: 19.8449\n",
            "Epoch 10/15\n",
            "194/194 [==============================] - 314s 2s/step - loss: 0.0171 - psnr: 19.8128 - val_loss: 0.0169 - val_psnr: 19.7650\n",
            "Epoch 11/15\n",
            "194/194 [==============================] - 314s 2s/step - loss: 0.0164 - psnr: 19.9567 - val_loss: 0.0163 - val_psnr: 19.9583\n",
            "Epoch 12/15\n",
            "194/194 [==============================] - 313s 2s/step - loss: 0.0160 - psnr: 20.0648 - val_loss: 0.0160 - val_psnr: 20.0499\n",
            "Epoch 13/15\n",
            "194/194 [==============================] - 313s 2s/step - loss: 0.0153 - psnr: 20.2468 - val_loss: 0.0164 - val_psnr: 20.0369\n",
            "Epoch 14/15\n",
            "194/194 [==============================] - 312s 2s/step - loss: 0.0161 - psnr: 20.0218 - val_loss: 0.0183 - val_psnr: 19.3594\n",
            "Epoch 15/15\n",
            "194/194 [==============================] - 329s 2s/step - loss: 0.0151 - psnr: 20.2601 - val_loss: 0.0158 - val_psnr: 20.3256\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e0ac0579000>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = RIDNet.evaluate(test_dataloader)\n",
        "print(f\"Test Loss: {results[0]}\")\n",
        "print(f\"Test PSNR: {results[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K9WYobMcFjh",
        "outputId": "9aa66acc-1e22-4be0-a37b-f1c9c79c8882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "97/97 [==============================] - 24s 247ms/step - loss: 0.0161 - psnr: 19.9635\n",
            "Test Loss: 0.016132060438394547\n",
            "Test PSNR: 19.963502883911133\n"
          ]
        }
      ]
    }
  ]
}